---
title: "Brandt et al. example: equivalent & not significant"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

## Equivalence bounds based on critical effect sizes

```{r include = FALSE}
library(TOSTER)
library(pwr)

# original study: Banerjee et al., study 1
orig.m1 <- 4.71
orig.m2 <- 5.3
orig.sd1 <- 0.85
orig.sd2 <- 0.97
orig.d <- 0.65
orig.t <- 2.03
orig.p <- 0.049
orig.N <- 40 #group size unknown, therefore equal n assumed
orig.df <- orig.N-2

# replication study: Brandt et al., study 1
rep.m1 <- 4.7857
rep.m2 <- 4.6569
rep.sd1 <- 1.0897
rep.sd2 <- 1.1895
rep.n1 <- 49
rep.n2 <- 51
rep.t <- 0.56
rep.p <- .574
rep.df <- rep.n1 + rep.n2 - 2
rep.d <- 0.11


# Calculate small telescopes effect (d) effect orig. study had 33% power to detect
d.33 <- (pwr.t.test(n = orig.N/2, d = NULL, sig.level = 0.05, power = 0.33, type = "two.sample", alternative = "two.sided"))$d

Brandt <- TOSTtwo(m1=rep.m1, m2=rep.m2, sd1=rep.sd1, sd2 = rep.sd2, n1 = rep.n1, n2=rep.n2, low_eqbound_d = -d.33, high_eqbound_d = d.33, var.equal = FALSE)
```

Another justifiable choice we would like to propose is to use the smallest observed effect size that could have been statistically significant in the original study. Based only on the alpha level and the sample size, we can calculate the criticial test value (e.g., $t$, $F$, $Z$). This critical test value can be transformed to a standardized effect size (e.g., $d_{crit} = t_{crit} \sqrt { \frac { 1} { n _ { 1} } + \frac { 1} { n _ { 2} } }$), which can thus be interpreted as a *critical effect size*[^1]. Observed effect sizes smaller than the critical effect size would not have been statistically significant in the original study. Based on this we can guess that the authors were not interested in effects smaller than this critical effect size, and thus use it as the SESOI. An equivalence test with these bounds can reject all observed effect sizes that the original study had the power to detect. 





Banerjee, Chatterjee, & Sinha (2012) reported that participants who had been asked to describe an unethical deed from their past judged the room to be darker than participants who had been asked to describe an ethical deed ($M_{unethical}= `r orig.m1`$, ${SD}_{unethical}= `r orig.sd1`$, $M_{ethical}=`r orig.m2`$, ${SD}_{ethical}=`r orig.sd2`$, $t(`r orig.df`)= `r orig.t`$, $p= `r orig.p`$, $d= `r orig.d`$). A close replication by Brandt, IJzerman, & Blanken (2014) found no significant effect ($t(`r rep.df`)=`r rep.t`$, $p=`r rep.p`$, $d=`r rep.d`$). Following the small telescopes approach, we can calculate the effect size the original study had $33\%$ power to detect --- $d = `r round(d.33, 2)`$ --- and use this as our SESOI. If we run a TOST with Welch's *t*-test for independent samples and equivalence bounds of ${\Delta}_L=`r round(-d.33, 2)`$ and ${\Delta}_U=`r round(d.33, 2)`$, we indeed find that the effect reported by the replication study is statistically equivalent, $t(`r round(Brandt$TOST_df, 2)`)=`r round(min(Brandt$TOST_t1, Brandt$TOST_t2), 2)`$, $p=`r round(max(Brandt$TOST_p1, Brandt$TOST_p2), 3)`$.





