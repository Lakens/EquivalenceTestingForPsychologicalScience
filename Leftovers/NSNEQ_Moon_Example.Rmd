---
title: "NSNEQ_Moon_Example"
author: "Peder M Isager"
date: "2017 M10 16"
output:
  html_document: default
  pdf_document: default
---

```{r include=FALSE}


#  Load the relevant datafile into R
moon.data        <- read.csv("NSNEQ_Moon_Example_data.csv")  # Data can be found at https://osf.io/rwv5s/ (final_dataset_139_to_send.sav)

#  Set the bounds, in this case 1 grade point, which corresponds to 6.25%
sesoi <- 0.0625

#  Calculate the summary statistics from the data that we need in the TOST calculation below
mean.asian   <- mean(moon.data$accuracy[moon.data$identity_salience == 1])
mean.control <- mean(moon.data$accuracy[moon.data$identity_salience == 2])
sd.asian         <- sd(moon.data$accuracy[moon.data$identity_salience == 1])
sd.control   <- sd(moon.data$accuracy[moon.data$identity_salience == 2])
n.asian          <- sum(moon.data$identity_salience == 1)
n.control        <- sum(moon.data$identity_salience == 2)

df <- (sd.asian^2 / n.asian + sd.control^2 / n.control)^2 / (((sd.asian^2 / n.asian)^2 / (n.asian-1)) + ((sd.control^2 / n.control)^2 / (n.control - 1)))  # Degrees of freedom for Welch's t-test
sample.sd <- sqrt((sd.asian^2 + sd.control^2) / 2)  # Calculate sd root mean squared for Welch's t-test
sample.se <- sqrt(sd.asian^2 / n.asian + sd.control^2 / n.control)  # Calculate the se of the difference based on sd.pooled

t <- (mean.asian - mean.control) / sample.se  # Welch t-test

d <- (mean.asian - mean.control) / sqrt((((n.asian - 1) * (sd.asian^2)) + (n.control - 1) * (sd.control^2)) / ((n.control + n.control) - 2))

p <- 2 * pt(t, df)

sdpooled <- sqrt((((n.asian - 1)*(sd.asian^2)) + (n.control - 1)*(sd.control^2))/((n.asian+n.control)-2))
d <- (mean.asian - mean.control) / sdpooled

#  We can now run the TOST!
#  Performing the TOST using the 'TOSTtwo.raw' function allows you to specify equivalence bounds in raw values
tost <- TOSTER::TOSTtwo.raw(m1  = mean.asian,
                                m2  = mean.control,
                                sd1 = sd.asian,
                                sd2 = sd.control,
                                n1  = n.asian,
                                n2  = n.control,
                                low_eqbound  = -sesoi,
                                high_eqbound =  sesoi,
                                alpha = .05)

```

Moon & Roeder (2014), replicating Shih, Pittinsky, and Ambady (1999), conducted a study to investigate whether Asian-American women would perform better on a maths test when primed with their Asian identity. They observed a slightly reversed difference between the Asian primed group (*n* = 53, *M* = 0.46, *SD* = 0.17) and the control (*n* = 48, *M* = 0.50, *SD* - 0.18) which was not significant, d = -0.23, *t*(96.62) = 1.15, *p* = 0.26, two-sided). A non-significant null-hypothesis test leaves two possible conclusions:

1. The data suggest the absence of a meaningful effect, and we can reject effects large enough to be meaningful.
2. The data are not sensitive enough to tell us whether a meaningful effect is present or absent.

Let us imagine that we consult a teacher about the test, and discover that that grades for this test are set at every 6.25% increase in correct answers (F = 0% to 6.25% ... A+ = 93.75% to 100%). We can decide that we are only interested in test differences so far as they correspond to at least a 1 grade point increase or decrease. Thus, our SESOI becomes a difference in raw scores of 6.25%, or 0.0625.

We then calculate the TOST for a two-sample Welch's t-test, assuming an alpha of 0.05 (two-sided), and using +/- the SESOI of 0.0625 as our equivalence bounds ($\Delta$).


We find that the TOST is non-significant, t(`r round(tost$TOST_df, 2)`) = `r round(min(abs(c(tost$TOST_t1, tost$TOST_t2))), 2)`, p = `r round(max(tost$TOST_p1, tost$TOST_p2), 2)` (figure 2C). Thus, we cannot reject the possibility that the true effect really is larger than our SESOI.[^footnote] When results are neither statistically different from zero nor statistically equivalent, there is insufficient data to draw conclusions. Further studies are needed, which can be analyzed using a (small-scale) meta-analysis. The additional data will narrow the confidence interval around the observed effect, allowing us to reject the null, reject the SESOI, or both. Analogous to the large sample sizes needed to detect small effects, a lot of data is needed to reject the SESOI when very narrow bounds are used for the equivalence test (e.g., $\Delta_{lower}$ = -0.1 and $\Delta_{upper}$ = 0.1). 

[^footnote]: In this example, the bounds of the TOST was set based on a raw effect of 6.25%. We could have converted this raw effect into a standardized effect (Cohen's d) for the current sample, and we would have gotten the exact same result, only then in Cohen's d ($\Delta = +/- `r round(0.0625 / sqrt((sd.asian^2 + sd.control^2) / 2), 2)`)$. Since this is a replication of a previous study, we could also have chosen to calculate the Cohen's d that corresponds to the raw SESOI in the *original* study, and used this as the bounds for the replication. Now we would no longer be answering a question about a raw effect independent of the samples. Instead, we are asking whether we in the replication can reject standardized effects larger than a certain standardized effect in the original study. Since the standard deviations of the original study were only slightly higher than in the replication in this case, Cohen's d (and thus the bounds) would also be only slightly smaller ($\Delta = +/- `r round(0.0625 / sqrt((0.17^2 + 0.20^2) / 2), 2)`)$. As standard deviations of the original study compared to the replication become larger, the bounds become more narrow.
