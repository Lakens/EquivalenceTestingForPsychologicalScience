
@article{Williams2008,
  title = {Experiencing {{Physical Warmth Promotes Interpersonal Warmth}}},
  volume = {322},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.1162548},
  language = {en},
  number = {5901},
  journal = {Science},
  author = {Williams, Lawrence E. and Bargh, John A.},
  month = oct,
  year = {2008},
  pages = {606--607}
}

@article{Bauer1996,
  title = {A Unifying Approach for Confidence Intervals and Testing of Equivalence and Difference},
  volume = {83},
  number = {4},
  journal = {Biometrika},
  author = {Bauer, Peter and Kieser, Meinhard},
  year = {1996},
  pages = {934--937}
}

@article{Brandt2014,
  title = {Does {{Recalling Moral Behavior Change}} the {{Perception}} of {{Brightness}}?: {{A Replication}} and {{Meta}}-{{Analysis}} of {{Banerjee}}, {{Chatterjee}}, and {{Sinha}} (2012)},
  volume = {45},
  issn = {1864-9335, 2151-2590},
  shorttitle = {Does {{Recalling Moral Behavior Change}} the {{Perception}} of {{Brightness}}?},
  doi = {10.1027/1864-9335/a000191},
  language = {en},
  number = {3},
  journal = {Social Psychology},
  author = {Brandt, Mark J. and IJzerman, Hans and Blanken, Irene},
  month = may,
  year = {2014},
  pages = {246--252}
}

@article{Baguley2009,
  title = {Standardized or Simple Effect Size: {{What}} Should Be Reported?},
  volume = {100},
  issn = {00071269},
  shorttitle = {Standardized or Simple Effect Size},
  doi = {10.1348/000712608X377117},
  language = {en},
  number = {3},
  journal = {British Journal of Psychology},
  author = {Baguley, Thom},
  month = aug,
  year = {2009},
  pages = {603--617}
}

@article{Banerjee2012,
  title = {Is {{It Light}} or {{Dark}}? {{Recalling Moral Behavior Changes Perception}} of {{Brightness}}},
  volume = {23},
  issn = {0956-7976, 1467-9280},
  shorttitle = {Is {{It Light}} or {{Dark}}?},
  doi = {10.1177/0956797611432497},
  language = {en},
  number = {4},
  journal = {Psychological Science},
  author = {Banerjee, Pronobesh and Chatterjee, Promothesh and Sinha, Jayati},
  month = apr,
  year = {2012},
  pages = {407--409}
}

@book{Chow2007,
  address = {Boca Raton},
  edition = {2 edition},
  title = {Sample {{Size Calculations}} in {{Clinical Research}}, {{Second Edition}}},
  isbn = {978-1-58488-982-3},
  abstract = {Focusing on an integral part of pharmaceutical development, Sample Size Calculations in Clinical Research, Second Edition presents statistical procedures for performing sample size calculations during various phases of clinical research and development. It provides sample size formulas and procedures for testing equality, noninferiority/superiority, and equivalence.    A comprehensive and unified presentation of statistical concepts and practical applications, this book highlights the interactions between clinicians and biostatisticians, includes a well-balanced summary of current and emerging clinical issues, and explores recently developed statistical methodologies for sample size calculation. Whenever possible, each chapter provides a brief history or background, regulatory requirements, statistical designs and methods for data analysis, real-world examples, future research developments, and related references.    One of the few books to systematically summarize clinical research procedures, this edition contains new chapters that focus on three key areas of this field. Incorporating the material of this book in your work will help ensure the validity and, ultimately, the success of your clinical studies.},
  language = {English},
  publisher = {{Chapman and Hall/CRC}},
  author = {Chow, Shein-Chung and Wang, Hansheng and Shao, Jun},
  month = aug,
  year = {2007}
}

@article{Hauck1984,
  title = {A New Statistical Procedure for Testing Equivalence in Two-Group Comparative Bioavailability Trials},
  volume = {12},
  issn = {0090-466X},
  doi = {10.1007/BF01063612},
  abstract = {The clinical problem of testing for equivalence in comparative bioavailability trials is restated in terms of the proper statistical hypotheses. A simple t-test procedure for these hypotheses has been devloped that is more powerful than the methods based on usual (shortest) and symmetric confidence intervals. In this note, this new procedure is explained and an example is given, including the method for sample size determination.},
  language = {en},
  number = {1},
  journal = {Journal of Pharmacokinetics and Biopharmaceutics},
  author = {Hauck, Dr Walter W. and Anderson, Sharon},
  month = feb,
  year = {1984},
  keywords = {Biochemistry; general,bioequivalence,Pharmacy,bioavailability,sample size determination,hypothesis tests,Veterinary Medicine,Pharmacology/Toxicology,Biomedical Engineering},
  pages = {83--91}
}

@article{Chen2000,
  title = {Tests for {{Equivalence}} or {{Noninferiority}} between {{Two Proportions}}},
  volume = {34},
  issn = {0092-8615},
  doi = {10.1177/009286150003400225},
  abstract = {Bioequivalence between two treatments or two drugs is often assessed by comparing the two proportions (success rate or eradication rate) of binomial outcomes when the conventional pharmacokinetic parameters are inadequate for the assessment. Setting the equivalence limits can be based on one of the three measures: difference, ratio, or odds ratio between the two binomial probabilities. This paper reviews the existing asymptotic test statistics for comparing two independent binomial probabilities in terms of the three measures in the context of equivalence or noninferiority testing. The actual type I error and power of the asymptotic tests are evaluated by enumerating the exact probabilities in the rejection region. The results show that to establish an equivalence between two treatments with an equivalence limit of 20\% in difference, a sample size of at least 50 per treatment is needed. When the sample size is sufficient, the actual type I error rate is close to the nominal level (slightly above the nominal level in several cases) for a test in terms of difference for equivalence limits, and it tends to exceed the nominal level for tests in terms of ratio or odds ratio.},
  language = {en},
  number = {2},
  journal = {Drug Information Journal},
  author = {Chen, James J. and Tsong, Yi and Kang, Seung-Ho},
  month = apr,
  year = {2000},
  pages = {569--578}
}

@book{Julious2010,
  address = {Boca Raton},
  title = {Sample Sizes for Clinical Trials},
  isbn = {978-1-58488-739-3},
  lccn = {R853.C55 J85 2010},
  publisher = {{CRC Press/Taylor \& Francis}},
  author = {Julious, Steven A.},
  year = {2010},
  keywords = {methods,Statistical methods,Clinical Trials as Topic,Sampling (Statistics),Sample Size,Clinical trials},
  note = {OCLC: ocn288983232},
  annote = {Seven key steps to cook up a sample size -- Sample sizes for parallel group superiority trials with normal data -- Sample size calculations for superiority cross-over trials with normal data -- Sample size calculations for equivalence clinical trials with normal data -- Sample size calculations for non-inferiority clinical trials with normal data -- Sample size calculations for bioequivalence trials -- Sample size calculations for precision clinical trials with normal data -- Sample size calculations for parallel group superiority clinical trials with binary data -- Sample size calculations for superiority cross-over clinical trials with binary data -- Sample size calculations for non-inferiority trials with binary data -- Sample size calculations for equivalence trials with binary data -- Sample size calculations for precision trials with binary data -- Sample size calculations for clinical trials with ordinal data -- Sample size calculations for clinical trials with survival data}
}

@article{Burriss2015,
  title = {Changes in {{Women}}'s {{Facial Skin Color}} over the {{Ovulatory Cycle}} Are {{Not Detectable}} by the {{Human Visual System}}},
  volume = {10},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0130093},
  abstract = {Human ovulation is not advertised, as it is in several primate species, by conspicuous sexual swellings. However, there is increasing evidence that the attractiveness of women's body odor, voice, and facial appearance peak during the fertile phase of their ovulatory cycle. Cycle effects on facial attractiveness may be underpinned by changes in facial skin color, but it is not clear if skin color varies cyclically in humans or if any changes are detectable. To test these questions we photographed women daily for at least one cycle. Changes in facial skin redness and luminance were then quantified by mapping the digital images to human long, medium, and shortwave visual receptors. We find cyclic variation in skin redness, but not luminance. Redness decreases rapidly after menstrual onset, increases in the days before ovulation, and remains high through the luteal phase. However, we also show that this variation is unlikely to be detectable by the human visual system. We conclude that changes in skin color are not responsible for the effects of the ovulatory cycle on women's attractiveness.},
  number = {7},
  journal = {PLOS ONE},
  author = {Burriss, Robert P. and Troscianko, Jolyon and Lovell, P. George and Fulford, Anthony J. C. and Stevens, Martin and Quigley, Rachael and Payne, Jenny and Saxton, Tamsin K. and Rowland, Hannah M.},
  month = jul,
  year = {2015},
  keywords = {Face,Color vision,Estrogens,Visual system,Ovulation,Cameras,Estradiol,Luminance},
  pages = {e0130093}
}

@article{Hoenig2001,
  title = {The Abuse of Power: The Pervasive Fallacy of Power Calculations for Data Analysis},
  volume = {55},
  shorttitle = {The Abuse of Power},
  number = {1},
  journal = {The American Statistician},
  author = {Hoenig, John M. and Heisey, Dennis M.},
  year = {2001},
  pages = {19--24}
}

@article{Dienes2014,
  title = {Using {{Bayes}} to Get the Most out of Non-Significant Results},
  volume = {5},
  doi = {10.3389/fpsyg.2014.00781},
  abstract = {No scientific conclusion follows automatically from a statistically non-significant result, yet people routinely use non-significant results to guide conclusions about the status of theories (or the effectiveness of practices). To know whether a non-significant result counts against a theory, or if it just indicates data insensitivity, researchers must use one of: power, intervals (such as confidence or credibility intervals), or else an indicator of the relative evidence for one theory over another, such as a Bayes factor. I argue Bayes factors allow theory to be linked to data in a way that overcomes the weaknesses of the other approaches. Specifically, Bayes factors use the data themselves to determine their sensitivity in distinguishing theories (unlike power), and they make use of those aspects of a theory's predictions that are often easiest to specify (unlike power and intervals, which require specifying the minimal interesting value in order to address theory). Bayes factors provide a coherent approach to determining whether non-significant results support a null hypothesis over a theory, or whether the data are just insensitive. They allow accepting and rejecting the null hypothesis to be put on an equal footing. Concrete examples are provided to indicate the range of application of a simple online Bayes calculator, which reveal both the strengths and weaknesses of Bayes factors.},
  journal = {Quantitative Psychology and Measurement},
  author = {Dienes, Zoltan},
  year = {2014},
  keywords = {Statistical inference,null hypothesis,highest density region,confidence interval,significance testing,Bayes factor,power},
  pages = {781}
}

@article{Button2015,
  title = {Minimal Clinically Important Difference on the {{Beck Depression Inventory}} - {{II}} According to the Patient's Perspective},
  volume = {45},
  issn = {0033-2917, 1469-8978},
  doi = {10.1017/S0033291715001270},
  abstract = {Background
The Beck Depression Inventory, 2nd edition (BDI-II) is widely used in research on depression. However, the minimal clinically important difference (MCID) is unknown. MCID can be estimated in several ways. Here we take a patient-centred approach, anchoring the change on the BDI-II to the patient's global report of improvement.

Method
 We used data collected (n = 1039) from three randomized controlled trials for the management of depression. Improvement on a `global rating of change' question was compared with changes in BDI-II scores using general linear modelling to explore baseline dependency, assessing whether MCID is best measured in absolute terms (i.e. difference) or as percent reduction in scores from baseline (i.e. ratio), and receiver operator characteristics (ROC) to estimate MCID according to the optimal threshold above which individuals report feeling `better'.

Results
Improvement in BDI-II scores associated with reporting feeling `better' depended on initial depression severity, and statistical modelling indicated that MCID is best measured on a ratio scale as a percentage reduction of score. We estimated a MCID of a 17.5\% reduction in scores from baseline from ROC analyses. The corresponding estimate for individuals with longer duration depression who had not responded to antidepressants was higher at 32\%.

Conclusions
MCID on the BDI-II is dependent on baseline severity, is best measured on a ratio scale, and the MCID for treatment-resistant depression is larger than that for more typical depression. This has important implications for clinical trials and practice.},
  number = {15},
  journal = {Psychological Medicine},
  author = {Button, K. S. and Kounali, D. and Thomas, L. and Wiles, N. J. and Peters, T. J. and Welton, N. J. and Ades, A. E. and Lewis, G.},
  month = nov,
  year = {2015},
  keywords = {primary care,minimal clinically important difference,2nd edition (BDI-II),depression,outcome assessment,Beck Depression Inventory},
  pages = {3269--3279}
}

@article{Dienes2017,
  title = {Four Reasons to Prefer {{Bayesian}} Analyses over Significance Testing},
  issn = {1069-9384, 1531-5320},
  doi = {10.3758/s13423-017-1266-z},
  abstract = {Inference using significance testing and Bayes factors is compared and contrasted in five case studies based on real research. The first study illustrates that the methods will often agree, both in motivating researchers to conclude that H1 is supported better than H0, and the other way round, that H0 is better supported than H1. The next four, however, show that the methods will also often disagree. In these cases, the aim of the paper will be to motivate the sensible evidential conclusion, and then see which approach matches those intuitions. Specifically, it is shown that a high-powered non-significant result is consistent with no evidence for H0 over H1 worth mentioning, which a Bayes factor can show, and, conversely, that a low-powered non-significant result is consistent with substantial evidence for H0 over H1, again indicated by Bayesian analyses. The fourth study illustrates that a high-powered significant result may not amount to any evidence for H1 over H0, matching the Bayesian conclusion. Finally, the fifth study illustrates that different theories can be evidentially supported to different degrees by the same data; a fact that P-values cannot reflect but Bayes factors can. It is argued that appropriate conclusions match the Bayesian inferences, but not those based on significance testing, where they disagree.},
  language = {en},
  journal = {Psychonomic Bulletin \& Review},
  author = {Dienes, Zoltan and McLatchie, Neil},
  month = mar,
  year = {2017},
  pages = {1--12}
}

@book{Cohen1988,
  address = {Hillsdale, N.J},
  edition = {2nd ed},
  title = {Statistical Power Analysis for the Behavioral Sciences},
  isbn = {978-0-8058-0283-2},
  lccn = {HA29 .C66 1988},
  publisher = {{L. Erlbaum Associates}},
  author = {Cohen, Jacob},
  year = {1988},
  keywords = {Statistical methods,Probabilities,Social sciences,Statistical power analysis},
  annote = {Includes index}
}

@article{Kahane2015,
  title = {`{{Utilitarian}}' Judgments in Sacrificial Moral Dilemmas Do Not Reflect Impartial Concern for the Greater Good},
  volume = {134},
  issn = {00100277},
  doi = {10.1016/j.cognition.2014.10.005},
  language = {en},
  journal = {Cognition},
  author = {Kahane, Guy and Everett, Jim A.C. and Earp, Brian D. and Farias, Miguel and Savulescu, Julian},
  month = jan,
  year = {2015},
  pages = {193--209}
}

@article{Hyde2008,
  title = {Gender {{Similarities Characterize Math Performance}}},
  volume = {321},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.1160364},
  language = {en},
  number = {5888},
  journal = {Science},
  author = {Hyde, J. S. and Lindberg, S. M. and Linn, M. C. and Ellis, A. B. and Williams, C. C.},
  month = jul,
  year = {2008},
  pages = {494--495}
}

@article{Kordsmeyer2017,
  title = {The Association of Three Indicators of Developmental Instability with Mating Success in Humans},
  volume = {38},
  issn = {10905138},
  doi = {10.1016/j.evolhumbehav.2017.08.002},
  language = {en},
  number = {6},
  journal = {Evolution and Human Behavior},
  author = {Kordsmeyer, Tobias L. and Penke, Lars},
  month = nov,
  year = {2017},
  pages = {704--713}
}

@article{Detrie1975,
  title = {{[Letter: The defense of French medical publications. I protest!]}},
  volume = {16},
  issn = {0003-4061},
  shorttitle = {{[Letter}},
  language = {fre},
  number = {2},
  journal = {Annales De L'anesthesiologie Francaise},
  author = {Detrie, P.},
  year = {1975 Mar-Apr},
  keywords = {France,Medicine,Periodicals as Topic},
  pages = {XLVIII--XLIX},
  pmid = {5000}
}

@book{Liu2008,
  series = {Chapman \& Hall/CRC Biostatistics Series},
  title = {Design and {{Analysis}} of {{Bioavailability}} and {{Bioequivalence Studies}}, {{Third Edition}}},
  volume = {3},
  isbn = {978-1-58488-668-6 978-1-4200-1167-8},
  language = {en},
  publisher = {{Chapman and Hall/CRC}},
  author = {Liu, Jen-pei and Chow, Shein-Chung},
  month = oct,
  year = {2008}
}

@article{Mara2012,
  title = {Paired-{{Samples Tests}} of {{Equivalence}}},
  volume = {41},
  issn = {0361-0918},
  doi = {10.1080/03610918.2011.626545},
  abstract = {Equivalence tests are used when the objective is to find that two or more groups are nearly equivalent on some outcome, such that any difference is inconsequential. Equivalence tests are available for several research designs, however, paired-samples equivalence tests that are accessible and relevant to the research performed by psychologists have been understudied. This study evaluated parametric and nonparametric two one-sided paired-samples equivalence tests and a standardized paired-samples equivalence test developed by Wellek (2003). The two one-sided procedures had better Type I error control and greater power than Wellek's test, with the nonparametric procedure having increased power with non normal distributions.},
  number = {10},
  journal = {Communications in Statistics - Simulation and Computation},
  author = {Mara, Constance A. and Cribbie, Robert A.},
  month = nov,
  year = {2012},
  pages = {1928--1943}
}

@article{Meyners2012,
  title = {Equivalence Tests \textendash{} {{A}} Review},
  volume = {26},
  issn = {09503293},
  doi = {10.1016/j.foodqual.2012.05.003},
  language = {en},
  number = {2},
  journal = {Food Quality and Preference},
  author = {Meyners, Michael},
  month = dec,
  year = {2012},
  pages = {231--245}
}

@article{Schuirmann1987,
  title = {A Comparison of the Two One-Sided Tests Procedure and the Power Approach for Assessing the Equivalence of Average Bioavailability},
  volume = {15},
  number = {6},
  journal = {Journal of pharmacokinetics and biopharmaceutics},
  author = {Schuirmann, Donald J.},
  year = {1987},
  pages = {657--680}
}

@article{Platt1964,
  title = {Strong Inference},
  volume = {146},
  number = {3642},
  journal = {science},
  author = {Platt, John R.},
  year = {1964},
  pages = {347--353}
}

@book{Machin2008,
  address = {Chichester, West Sussex, UK ; Hoboken, NJ},
  edition = {3rd ed},
  title = {Sample Size Tables for Clinical Studies},
  isbn = {978-1-4051-4650-0},
  lccn = {R853.C55 S36 2008},
  publisher = {{Wiley-Blackwell}},
  editor = {Machin, David},
  year = {2008},
  keywords = {Statistics as Topic,Statistical methods,Clinical Trials as Topic,Research Design,Sample Size,Clinical trials},
  annote = {Basic design considerations -- Distributions and confidence intervals -- Comparing two independent groups for binary data -- Comparing two independent groups for ordered categorical data -- Comparing two independent groups for continuous data -- Cluster designs, repeated measures data and more than two groups -- Comparing paired groups for binary, ordered categorical and continuous outcomes -- Comparing survival curves -- Equivalence -- Confidence intervals -- Post-marketing surveillance -- The correlation coefficient -- Reference intervals and receiver operating curves -- Observer agreement studies -- Dose finding studies -- Phase II trials -- Sample size software (SSS)}
}

@article{Lenth2001,
  title = {Some Practical Guidelines for Effective Sample Size Determination},
  volume = {55},
  number = {3},
  journal = {The American Statistician},
  author = {Lenth, Russell V.},
  year = {2001},
  pages = {187--193}
}

@article{Lenth2007,
  title = {Post Hoc Power: Tables and Commentary},
  shorttitle = {Post Hoc Power},
  journal = {Iowa City: Department of Statistics and Actuarial Science, University of Iowa},
  author = {Lenth, Russell V.},
  year = {2007}
}

@book{Murphy2014,
  title = {Statistical {{Power Analysis}}: {{A Simple}} and {{General Model}} for {{Traditional}} and {{Modern Hypothesis Tests}}, {{Fourth Edition}}},
  isbn = {978-1-317-68057-4},
  shorttitle = {Statistical {{Power Analysis}}},
  abstract = {Noted for its accessible approach, this text applies the latest approaches of power analysis to both null hypothesis and minimum-effect testing using the same basic unified model. Through the use of a few simple procedures and examples, the authors show readers with little expertise in statistical analysis how to obtain the values needed to carry out the power analysis for their research. Illustrations of how these analyses work and how they can be used to choose the appropriate criterion for defining statistically significant outcomes are sprinkled throughout. The book presents a simple and general model for statistical power analysis based on the F statistic and reviews how to determine: the sample size needed to achieve desired levels of power; the level of power needed in a study; the size of effect that can be reliably detected by a study; and sensible criteria for statistical significance. The book helps readers design studies, diagnose existing studies, and understand why hypothesis tests come out out the way they do.  The fourth edition features: -New Boxed Material sections provide examples of power analysis in action and discuss unique issues that arise as a result of applying power analyses in different designs. -Many more worked examples help readers apply the concepts presented. -Expanded coverage of power analysis for multifactor analysis of variance (ANOVA) to show readers how to analyze up to four factors with repeated measures on any or all of the factors. -Re-designed and expanded web based One Stop F Calculator software and data sets that allow users to perform all of the book's analyses and conduct significance tests, power analyses, and assessments of N and alpha needed for traditional and minimum-effects tests. -Easy to apply formulas for approximating the number of subjects required to reach adequate levels of power in a wide range of studies.  Intended as a supplement for graduate/advanced undergraduate courses in research methods or experimental design, intermediate, advanced, or multivariate statistics, statistics II, or psychometrics, taught in psychology, education, business, and other social and health sciences, researchers also appreciate the book`s applied approach.},
  language = {en},
  publisher = {{Routledge}},
  author = {Murphy, Kevin R. and Myors, Brett and Wolach, Allen},
  month = may,
  year = {2014},
  keywords = {Psychology / Research \& Methodology,Psychology / Statistics,Education / Statistics,Business \& Economics / Statistics}
}

@article{Schumann2017,
  title = {When Is Computer-Mediated Intergroup Contact Most Promising? {{Examining}} the Effect of out-Group Members' Anonymity on Prejudice},
  volume = {77},
  issn = {07475632},
  shorttitle = {When Is Computer-Mediated Intergroup Contact Most Promising?},
  doi = {10.1016/j.chb.2017.08.006},
  language = {en},
  journal = {Computers in Human Behavior},
  author = {Schumann, Sandy and Klein, Olivier and Douglas, Karen and Hewstone, Miles},
  month = dec,
  year = {2017},
  pages = {198--210}
}

@article{Maxwell2015,
  title = {Is Psychology Suffering from a Replication Crisis? {{What}} Does ``Failure to Replicate'' Really Mean?},
  volume = {70},
  issn = {1935-990X, 0003-066X},
  shorttitle = {Is Psychology Suffering from a Replication Crisis?},
  doi = {10.1037/a0039400},
  language = {en},
  number = {6},
  journal = {American Psychologist},
  author = {Maxwell, Scott E. and Lau, Michael Y. and Howard, George S.},
  year = {2015},
  pages = {487--498}
}

@article{Perugini2014,
  title = {Safeguard {{Power}} as a {{Protection Against Imprecise Power Estimates}}},
  volume = {9},
  issn = {1745-6916, 1745-6924},
  doi = {10.1177/1745691614528519},
  language = {en},
  number = {3},
  journal = {Perspectives on Psychological Science},
  author = {Perugini, Marco and Gallucci, Marcello and Costantini, Giulio},
  month = may,
  year = {2014},
  pages = {319--332}
}

@article{Lynott2014,
  title = {Replication of ``{{Experiencing Physical Warmth Promotes Interpersonal Warmth}}'' by {{Williams}} and {{Bargh}} (2008)},
  volume = {45},
  issn = {1864-9335, 2151-2590},
  doi = {10.1027/1864-9335/a000187},
  language = {en},
  number = {3},
  journal = {Social Psychology},
  author = {Lynott, Dermot and Corker, Katherine S. and Wortman, Jessica and Connell, Louise and Donnellan, M. Brent and Lucas, Richard E. and O'Brien, Kerry},
  month = may,
  year = {2014},
  pages = {216--222}
}

@article{Moon2014,
  title = {A {{Secondary Replication Attempt}} of {{Stereotype Susceptibility}} ({{Shih}}, {{Pittinsky}}, \& {{Ambady}}, 1999)},
  volume = {45},
  issn = {1864-9335, 2151-2590},
  doi = {10.1027/1864-9335/a000193},
  language = {en},
  number = {3},
  journal = {Social Psychology},
  author = {Moon, Alice and Roeder, Scott S.},
  month = may,
  year = {2014},
  pages = {199--201}
}

@article{TunesdaSilva2008,
  title = {Methods for {{Equivalence}} and {{Noninferiority Testing}}},
  volume = {15},
  issn = {1083-8791},
  doi = {10.1016/j.bbmt.2008.10.004},
  abstract = {Classical hypothesis testing focuses on testing whether treatments have differential effects on outcome. However, sometimes clinicians may be more interested in determining whether treatments are equivalent or whether one has noninferior outcomes. We review the hypotheses for these noninferiority and equivalence research questions, consider power and sample size issues, and discuss how to perform such a test for both binary and survival outcomes. The methods are illustrated on 2 recent studies in hematopoietic cell transplantation.},
  number = {1 Suppl},
  journal = {Biology of blood and marrow transplantation : journal of the American Society for Blood and Marrow Transplantation},
  author = {{Tunes da Silva}, Gisela and Logan, Brent R. and Klein, John P.},
  month = jan,
  year = {2008},
  pages = {120--127},
  pmid = {19147090},
  pmcid = {PMC2701110}
}

@article{Tryon2008,
  title = {An Inferential Confidence Interval Method of Establishing Statistical Equivalence That Corrects {{Tryon}}'s (2001) Reduction Factor.},
  volume = {13},
  number = {3},
  journal = {Psychological methods},
  author = {Tryon, Warren W. and Lewis, Charles},
  year = {2008},
  pages = {272}
}

@article{Seaman1998,
  title = {Equivalence Confidence Intervals for Two-Group Comparisons of Means.},
  volume = {3},
  copyright = {\textcopyright{} American Psychological Association 1998},
  issn = {1082-989X},
  doi = {http://dx.doi.org.dianus.libr.tue.nl/10.1037/1082-989X.3.4.403},
  abstract = {Methods for determining whether 2 means are practically equivalent are discussed. Existing equivalency-testing procedures are reviewed and compared. An equivalence confidence interval is proposed and compared with the traditional confidence interval for a mean difference. The use of this new interval is described for both confirmatory and exploratory research and is shown to fit within the context of good-enough methods. Adaptations and extensions of the interval are proposed. (PsycINFO Database Record (c) 2013 APA, all rights reserved)(journal abstract)},
  language = {English},
  number = {4},
  journal = {Psychological Methods},
  author = {Seaman, Michael A. and Serlin, Ronald C.},
  month = dec,
  year = {1998},
  keywords = {Confidence Limits (Statistics) (major),Statistical Analysis (major),Mean (major)},
  pages = {403--411}
}

@book{Wellek2010,
  address = {Boca Raton},
  edition = {2nd ed},
  title = {Testing Statistical Hypotheses of Equivalence and Noninferiority},
  isbn = {978-1-4398-0818-4},
  lccn = {QA277 .W46 2010},
  publisher = {{CRC Press}},
  author = {Wellek, Stefan},
  year = {2010},
  keywords = {Statistical hypothesis testing},
  annote = {"A Chapman \& Hall book."}
}

@article{Senn1993,
  title = {Inherent Difficulties with Active Control Equivalence Studies},
  volume = {12},
  number = {24},
  journal = {Statistics in medicine},
  author = {Senn, Stephen},
  year = {1993},
  pages = {2367--2375}
}

@article{Simonsohn2015,
  title = {Small {{Telescopes Detectability}} and the {{Evaluation}} of {{Replication Results}}},
  volume = {26},
  issn = {0956-7976, 1467-9280},
  doi = {10.1177/0956797614567341},
  abstract = {This article introduces a new approach for evaluating replication results. It combines effect-size estimation with hypothesis testing, assessing the extent to which the replication results are consistent with an effect size big enough to have been detectable in the original study. The approach is demonstrated by examining replications of three well-known findings. Its benefits include the following: (a) differentiating ``unsuccessful'' replication attempts (i.e., studies yielding p $>$ .05) that are too noisy from those that actively indicate the effect is undetectably different from zero, (b) ``protecting'' true findings from underpowered replications, and (c) arriving at intuitively compelling inferences in general and for the revisited replications in particular.},
  language = {en},
  number = {5},
  journal = {Psychological Science},
  author = {Simonsohn, Uri},
  month = may,
  year = {2015},
  keywords = {replication,open materials,hypothesis testing,statistical power},
  pages = {559--569},
  pmid = {25800521}
}

@book{UnitedStates2016,
  address = {Washington, DC},
  title = {The {{CIA World Factbook}} 2017},
  isbn = {978-1-5107-1288-1},
  language = {English},
  author = {{United States} and {Central Intelligence Agency}},
  year = {2016},
  note = {OCLC: 936533494}
}

@article{Weber2012,
  title = {Testing {{Equivalence}} in {{Communication Research}}: {{Theory}} and {{Application}}},
  volume = {6},
  issn = {1931-2458},
  shorttitle = {Testing {{Equivalence}} in {{Communication Research}}},
  doi = {10.1080/19312458.2012.703834},
  abstract = {Although equivalence testing is preferred when a researcher's goal is to support the null hypothesis (i.e., no substantial effect), equivalence tests are virtually unknown and unused in the communication field. This article provides the rationale for and theoretical background of equivalence testing and offers examples of equivalence tests for the independent and dependent groups t-test and tests of association using Pearson's coefficient or correlation. From a review of meta-analyses, we provide tables of commonly observed effect-sizes across subdisciplines and topic areas in communication and offer these as a guideline for choosing minimum substantial effects ($\Delta$) in equivalence testing when no other information source is available. To facilitate the adoption of equivalence tests in future research, we provide easy-to-use custom dialogs for SPSS which greatly simplify their computation and application.},
  number = {3},
  journal = {Communication Methods and Measures},
  author = {Weber, Ren{\'e} and Popova, Lucy},
  month = jul,
  year = {2012},
  pages = {190--213}
}

@article{Shih1999,
  title = {Stereotype {{Susceptibility}}: {{Identity Salience}} and {{Shifts}} in {{Quantitative Performance}}},
  volume = {10},
  issn = {0956-7976, 1467-9280},
  shorttitle = {Stereotype {{Susceptibility}}},
  doi = {10.1111/1467-9280.00111},
  language = {en},
  number = {1},
  journal = {Psychological Science},
  author = {Shih, Margaret and Pittinsky, Todd L. and Ambady, Nalini},
  month = jan,
  year = {1999},
  pages = {80--83}
}

@article{1975,
  title = {V.{{I}}. {{Gavrilov}}},
  volume = {19},
  issn = {0001-723X},
  language = {eng},
  number = {6},
  journal = {Acta Virologica},
  author = {Gavrilov, V. I.},
  month = nov,
  year = {1975},
  keywords = {History; 20th Century,USSR,Virology},
  pages = {510},
  pmid = {2003}
}

@book{RStudioTeam2016,
  address = {Boston, MA},
  title = {{{RStudio}}: {{Integrated Development Environment}} for {{R}}},
  author = {{RStudio Team}},
  year = {2016},
  organization = {RStudio, Inc.}
}

@article{Brown2017,
  title = {Preliminary {{Evidence}} for {{How}} the {{Behavioral Immune System Predicts Juror Decision}}-{{Making}}},
  volume = {3},
  issn = {2198-9885},
  doi = {10.1007/s40806-017-0102-z},
  language = {en},
  number = {4},
  journal = {Evolutionary Psychological Science},
  author = {Brown, Mitch and Rodriguez, Dario N. and Gretak, Alyssa P. and Berry, Melissa A.},
  month = may,
  year = {2017},
  pages = {325--334}
}

@article{Hemphill2003,
  title = {Interpreting the Magnitudes of Correlation Coefficients.},
  volume = {58},
  number = {1},
  journal = {American Psychologist},
  author = {Hemphill, James F.},
  year = {2003},
  pages = {78--80}
}

@article{Lakens2017a,
  title = {Equivalence {{Tests}}: {{A Practical Primer}} for t {{Tests}}, {{Correlations}}, and {{Meta}}-{{Analyses}}},
  volume = {8},
  issn = {1948-5506},
  shorttitle = {Equivalence {{Tests}}},
  doi = {10.1177/1948550617697177},
  abstract = {Scientists should be able to provide support for the absence of a meaningful effect. Currently, researchers often incorrectly conclude an effect is absent based a nonsignificant result. A widely recommended approach within a frequentist framework is to test for equivalence. In equivalence tests, such as the two one-sided tests (TOST) procedure discussed in this article, an upper and lower equivalence bound is specified based on the smallest effect size of interest. The TOST procedure can be used to statistically reject the presence of effects large enough to be considered worthwhile. This practical primer with accompanying spreadsheet and R package enables psychologists to easily perform equivalence tests (and power analyses) by setting equivalence bounds based on standardized effect sizes and provides recommendations to prespecify equivalence bounds. Extending your statistical tool kit with equivalence tests is an easy way to improve your statistical and theoretical inferences.},
  language = {en},
  number = {4},
  journal = {Social Psychological and Personality Science},
  author = {Lakens, Daniel},
  month = may,
  year = {2017},
  pages = {355--362}
}

@article{Lakens2017b,
  title = {Justify {{Your Alpha}}: {{A Response}} to ``{{Redefine Statistical Significance}}''},
  shorttitle = {Justify {{Your Alpha}}},
  doi = {10.17605/OSF.IO/9S3Y6},
  abstract = {In response to recommendations to redefine statistical significance to p $\leq$ .005, we propose that researchers should transparently report and justify all choices they make when designing a study, including the alpha level.},
  journal = {PsyArXiv},
  author = {Lakens, Daniel and Adolfi, Federico G. and Albers, Casper and Anvari, Farid and Apps, Matthew A. J. and Argamon, Shlomo Engelson and van Assen, Marcel A. L. M. and Baguley, Thom and Becker, Raymond and Benning, Stephen D. and Bradford, Daniel E. and Buchanan, Erin Michelle and Caldwell, Aaron and van Calster, Ben and Carlsson, Rickard and Chen, Sau-Chin and Chung, Bryan and Colling, Lincoln and Collins, Gary and Crook, Zander and Cross, Emily S. and Daniels, Sameera and Danielsson, Henrik and DeBruine, Lisa and Dunleavy, Daniel and Earp, Brian D. and Feist, Michele and Ferrell, Jason D. and Field, James G. and Fox, Nick and Friesen, Amanda and Gomes, Caio and Grange, James A. and Grieve, Andrew and Guggenberger, Robert and Harmelen, Anne-Laura Van and Hasselman, Fred and Hochard, Kevin D. and Hoffarth, Mark Romeo and Holmes, Nicholas Paul and Ingre, Michael and Isager, Peder and Isotalus, Hanna and Johansson, Christer and Juszczyk, Konrad and Kenny, David and Khalil, Ahmed Abdelrahim and Konat, Barbara and Lao, Junpeng and Larsen, Erik Gahner and Lodder, Gerine M. A. and Lukavsky, Jiri and Madan, Christopher and Manheim, David and Gonzalez-Marquez, Monica and Martin, Stephen R. and Martin, Andrea E. and Mayo, Deborah and McCarthy, Randy J. and McConway, Kevin and McFarland, Colin and Nilsonne, Gustav and Nio, Amanda Q. X. and de Oliveira, Cilene Lino and Parsons, Sam and Pfuhl, Gerit and Quinn, Kimberly and Sakon, John and Saribay, Selahattin Adil and Schneider, Iris and Selvaraju, Manojkumar and Sjoerds, Zsuzsika and Smith, Samuel and Smits, Tim and Spies, Jeffrey R. and Sreekumar, Vishnu and Steltenpohl, Crystal and Stenhouse, Neil and {\'S}wi\k{a}tkowski, Wojciech and Vadillo, Miguel A. and Williams, Matt and Williams, Samantha and Williams, Donald R. and de Xivry, Jean-Jacques Orban and Yarkoni, Tal and Ziano, Ignazio and Zwaan, Rolf},
  month = sep,
  year = {2017}
}

@misc{jamovi2017,
  title = {Jamovi ({{Version}} 0.8) [{{Computer Software}}]. {{Retrieved}} from {{https://www.jamovi.org}}},
  author = {{jamovi project}},
  year = {2017}
}

@article{Tryon2001,
  title = {Evaluating Statistical Difference, Equivalence, and Indeterminacy Using Inferential Confidence Intervals: An Integrated Alternative Method of Conducting Null Hypothesis Statistical Tests.},
  volume = {6},
  shorttitle = {Evaluating Statistical Difference, Equivalence, and Indeterminacy Using Inferential Confidence Intervals},
  number = {4},
  journal = {Psychological methods},
  author = {Tryon, Warren W.},
  year = {2001},
  pages = {371--386}
}

@article{Piaggio2006,
  title = {Reporting of {{Noninferiority}} and {{Equivalence Randomized Trials}}: {{An Extension}} of the {{CONSORT Statement}}},
  volume = {295},
  issn = {0098-7484},
  shorttitle = {Reporting of {{Noninferiority}} and {{Equivalence Randomized Trials}}},
  doi = {10.1001/jama.295.10.1152},
  language = {en},
  number = {10},
  journal = {JAMA},
  author = {Piaggio, Gilda and Elbourne, Diana R. and Altman, Douglas G. and Pocock, Stuart J. and Evans, Stephen J. W. and the CONSORT Group, for},
  month = mar,
  year = {2006},
  pages = {1152--1160}
}

@article{Norman2003,
  title = {Interpretation of {{Changes}} in {{Health}}-Related {{Quality}} of {{Life}}: {{The Remarkable Universality}} of {{Half}} a {{Standard Deviation}}},
  volume = {41},
  issn = {0025-7079},
  abstract = {Background.  A number of studies have computed the minimally important difference (MID) for health-related quality of life instruments.
Objective.  To determine whether there is consistency in the magnitude of MID estimates from different instruments.
Methods.  We conducted a systematic review of the literature to identify studies that computed an MID and contained sufficient information to compute an effect size (ES). Thirty-eight studies fulfilled the criteria, resulting in 62 ESs.
Results.  For all but 6 studies, the MID estimates were close to one half a SD (mean = 0.495, SD = 0.155). There was no consistent relationship with factors such as disease-specific or generic instrument or the number of response options. Negative changes were not associated with larger ESs. Population-based estimation procedures and brief follow-up were associated with smaller ESs, and acute conditions with larger ESs. An explanation for this consistency is that research in psychology has shown that the limit of people's ability to discriminate over a wide range of tasks is approximately 1 part in 7, which is very close to half a SD.
Conclusion.  In most circumstances, the threshold of discrimination for changes in health-related quality of life for chronic diseases appears to be approximately half a SD.
The interpretation of changes in health-related quality of life (HRQL) has been a research focus for more than a decade. 1 More recently, researchers have been devising methods to identify a minimal level of change consistent with real, as opposed to statistically significant, benefit. 2
The determination of the minimal level of real change for any HRQL scale can be a daunting task. It may potentially vary for different questionnaires, different diseases, and different demographic groups. Potential influences related to the questionnaire itself include the relative position of the individual on the HRQL scale (ie, floor and ceiling effects 3), the number of steps on the scale, the number of items, and so forth. The intent of the analysis (making a diagnosis vs. testing the efficacy of an intervention) and the identity of the individual performing the assessment (patient vs. clinical staff) could potentially result in a different estimate of important change. Collectively, considering all these variables for each measure, this variation represents a prohibitive impediment to the successful implementation of HRQL endpoints in clinical research and practice.
However, there is some evidence that some of these various factors may have a relatively small impact on the magnitude of the minimal difference. Certainly, some authors have noted that, over a series of studies with a diversity of conditions and age groups using disease-specific measures with 7-point response scales, the minimally important difference (MID) appears to fall consistently close to 0.5 points on the 7-point scale. 1,4,5 It is the thesis of the present article that there is more commonality than difference in the variety of approaches. We will show that a multiplicity of methods, using several different scales from time tradeoff to visual analogue, with the number of items ranging from 1 to more than 50, in a diversity of chronic conditions, led to remarkably similar estimates. We will argue that this convergence is not accidental, but is a direct consequence of the limit of human discrimination ability. Finally, we will point out some circumstances that diverge from this consistency.},
  number = {5},
  journal = {Medical Care},
  author = {Norman, Geoffrey R. and Sloan, Jeff A. and Wyrwich, Kathleen W.},
  year = {2003},
  keywords = {effect size,interpretation,MID,threshold,Quality of life},
  pages = {582--592}
}

@techreport{Morey2016,
  title = {Why {{Most Of Psychology Is Statistically Unfalsifiable}}},
  abstract = {Version initially submitted},
  institution = {{Zenodo}},
  author = {Morey, Richard and Lakens, Daniel},
  month = oct,
  year = {2016},
  doi = {10.5281/zenodo.838685}
}

@article{Goertzen2010,
  title = {Detecting a Lack of Association: {{An}} Equivalence Testing Approach},
  volume = {63},
  copyright = {2010 The British Psychological Society},
  issn = {2044-8317},
  shorttitle = {Detecting a Lack of Association},
  doi = {10.1348/000711009X475853},
  abstract = {Researchers often test for a lack of association between variables. A lack of association is usually established by demonstrating a non-significant relationship with a traditional test (e.g., Pearson's r). However, for logical as well as statistical reasons, such conclusions are problematic. In this paper, we discuss and compare the empirical Type I error and power rates of three lack of association tests. The results indicate that large, sometimes very large, sample sizes are required for the test statistics to be appropriate. What is especially problematic is that the required sample sizes may exceed what is practically feasible for the conditions that are expected to be common among researchers in psychology. This paper highlights the importance of using available lack of association tests, instead of traditional tests of association, for demonstrating the independence of variables, and qualifies the conditions under which these tests are appropriate.},
  language = {en},
  number = {3},
  journal = {British Journal of Mathematical and Statistical Psychology},
  author = {Goertzen, Jason R. and Cribbie, Robert A.},
  month = nov,
  year = {2010},
  pages = {527--537}
}

@article{Rogers1993,
  title = {Using Significance Tests to Evaluate Equivalence between Two Experimental Groups.},
  volume = {113},
  number = {3},
  journal = {Psychological bulletin},
  author = {Rogers, James L. and Howard, Kenneth I. and Vessey, John T.},
  year = {1993},
  pages = {553}
}

@book{Senn2007,
  address = {Chichester, England ; Hoboken, NJ},
  edition = {2nd ed},
  series = {Statistics in practice},
  title = {Statistical Issues in Drug Development},
  isbn = {978-0-470-01877-4},
  lccn = {RM301.25 .S46 2007},
  publisher = {{John Wiley \& Sons}},
  author = {Senn, Stephen},
  year = {2007},
  keywords = {Clinical Trials as Topic,Drug Design,Drug development,methods,Statistical methods,Statistics as Topic},
  annote = {A brief and superficial history of statistics for drug developers -- Design and interpretation of clinical trials as seen by a statistician -- Probability, Bayes, P-values, tests of hypotheses, and confidence intervals -- The work of the pharmaceutical statistician -- Allocating treatments to patients in clinical trials -- Baselines and covariate information -- The measurement of treatment effects -- Demographic subgroups : representation and analysis -- Multiplicity -- Intention to treat, missing data and related matters -- One-sided and two-sided tests and other issues to do with significance and P-values -- Determining the sample size -- Multicentre trials -- Active control equivalence studies -- Meta-analysis -- Cross-over trials -- n-of-1 trials -- Sequential trials -- Dose-finding -- Concerning pharmacokinetics and pharmacodynamics -- Bioequivalence studies -- Safety data, harms, drug monitoring, and pharmaco-epidemiology -- Pharmaco-economics and portfolio management -- Concerning pharmacogenetics, pharmacogenomics, and related matters}
}

@article{Neyman1933,
  title = {On the {{Problem}} of the {{Most Efficient Tests}} of {{Statistical Hypotheses}}},
  volume = {231},
  issn = {1364-503X, 1471-2962},
  doi = {10.1098/rsta.1933.0009},
  language = {en},
  number = {694-706},
  journal = {Philosophical Transactions of the Royal Society of London A: Mathematical, Physical and Engineering Sciences},
  author = {Neyman, J. and Pearson, E. S.},
  month = jan,
  year = {1933},
  pages = {289--337}
}

@article{Meehl1978,
  title = {Theoretical Risks and Tabular Asterisks: {{Sir Karl}}, {{Sir Ronald}}, and the Slow Progress of Soft Psychology.},
  volume = {46},
  issn = {0022-006X},
  shorttitle = {Theoretical Risks and Tabular Asterisks},
  doi = {10.1037/0022-006X.46.4.806},
  language = {en},
  number = {4},
  journal = {Journal of Consulting and Clinical Psychology},
  author = {Meehl, Paul E.},
  year = {1978},
  pages = {806--834}
}

@article{Quertemont2011,
  title = {How to {{Statistically Show}} the {{Absence}} of an {{Effect}}},
  volume = {51},
  issn = {2054-670X, 0033-2879},
  doi = {10.5334/pb-51-2-109},
  number = {2},
  journal = {Psychologica Belgica},
  author = {Quertemont, Etienne},
  month = aug,
  year = {2011},
  pages = {109}
}

@article{Delacre2017,
  title = {Why {{Psychologists Should}} by {{Default Use Welch}}'s {\emph{t}}-Test {{Instead}} of {{Student}}'s {\emph{t}}-Test},
  volume = {30},
  issn = {2397-8570, 0992-986X},
  doi = {10.5334/irsp.82},
  number = {1},
  journal = {International Review of Social Psychology},
  author = {Delacre, Marie and Lakens, Daniel and Leys, Christophe},
  month = apr,
  year = {2017},
  pages = {92}
}


